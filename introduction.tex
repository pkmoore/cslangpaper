\section{Introduction}
\label{SEC:introduction}

\textit{But it works on my machine!} goes the refrain of the tortured
software developer whose application works correctly when executed on a
development computer only to fail once it is deployed.  This occurs with
such frequency that the ``works on my machine'' phenomenon is a well known
source of pain that is often discussed in software and project management
literature~\cite{notreal}.  The problem is so widespread that FAKESTUDY
concluded that \$XXX are spent annually on efforts to recall, fix, and
redeploy applications because of bugs that slipped past extensive testing
efforts during development.

Earlier work has demonstrated that environmental bugs, those that occur due
to unanticipated qualities external to an application, are a major source
of such failures.  This fact continues to be reinforced by the regular
appearance of high impact environmental bugs in major pieces of
software~\cite{devzeroroot}.  And it appears that no class of application
is safe with environmental bugs affecting operating systems~\cite{bad},
user applications~\cite{bad} and even web applications~\cite{bad} in the
last year alone!


%No matter how well an application is tested before its release,
%new bugs always seem to emerge after deployment.
%In fact, Oracle estimates that 40\% of deployed applications
%contain critical defects -- a fact that is compounded that deployment
%increases the cost to fix these flaws by 100 times~\cite{OracleAppQuality}.
%One reason for this behavior
%is that these applications will operate within a diverse set of
%deployment \emph{environments},
%and variations between these environments tend to
%reveal previously undiscovered flaws.
%These flaws emerge from
%such factors as
%operating system APIs changing across versions
%~\cite{LinuxGlibcChanges},
%or small variations in file systems exhibiting subtle but critical
%differences~\cite{EXT4Layout, AppleHFS}.
%Even if the network and adapter are identical,
%network behavior can still diverge from what is expected~\cite{vbox},
%and these environmental differences greatly exacerbate
%the chance that an application will function incorrectly when deployed.


%These unforeseeable bugs
%complicate the work of application developers who, according to a
%recent survey conducted by ClusterHQ~\cite{ClusterHQSurvey},
%spend a significant portion of their time
%debugging errors that only appear in production.
%Numerous efforts have been made to reduce this burden.
%One approach
%is to hide environmental differences behind standard interfaces.
%Unfortunately,
%even specialized ``Write-Once, Run Anywhere'' environments
%that attempt to hide these differences,
%such as the Java Runtime Environment,
%are not perfect,
%leading them to be rechristened ``Write-Once, Debug Everywhere''~\cite{WODE}.
%A more direct approach would be
%to identify and fix deficiencies before deployment,
%but history has shown that,
%even if enormous effort is put forward,
%it may be insufficient to uncover these bugs.
%Microsoft employs thousands of engineers with nearly a
%1:1 ratio of testers to developers~\cite{Page2009}.
%Yet, a recent Windows Update released
%in response to the Spectre Intel CPU vulnerability
%resulted in machines with certain hardware configurations
%being rendered unbootable~\cite{kb4056892}.


%twe introduced Simulating Environmental Anomalies (SEA)
%and its key insight that
%problematic environmental properties,
%which we refer to as \textit{anomalies},
%can often be detected
%in the function calls,
%system calls,
%or other communications an application makes within an environment.
%When employing SEA,
%the anomalies
%unique to a given environment
%can be inserted into
%the communications of an application under test
%in such a way
%that its responses will indicate potential
%for failures upon deployment.
%These anomalies can be preserved and cataloged
%allowing them to be reused to test other applications,
%without requiring
%per-application effort.
%Over time,
%a corpus of environmental anomalies can be assembled
%that can help ensure
%future applications do not suffer from bugs of the past.


%%% Introduce that there was a paper
%%% We made an initial thrust at this with our 2019 paper ...
%%% that introduced the SEA technique
%%% Look at OOPSLA and see if it is single blind or double blind

We made an initial thrust at this problem by employing the
Simulating Environmental Anomalies (SEA) technique on the system calls an
application makes. Our effort centered on the key insight, that problematic
environmental properties, known as anomalies, are visible in the
communications between the components that make up an application.
We found that, once captured, these anomalies allowed us to test other
applications to see how they would fare in a given environment.
% Combine these three sentences into something shorter
% that talks about how we are expanding the SEA technique
But system calls are just part of the story --
This paper documents our effort to augment SEA
so that its proven methodology
can be used to test a broader set of applications.
Our improvements
apply SEA's existing capability
to new bug domains
while maintaining the its existing advantages and
dramatically
improving its reach.
Central to this goal is
a more generic way to
encode and simulate anomalies across the set of applications that use
structured communications like remote procedure calls.
Our improvements allow anomalies that were
valuable in testing one application to be directly used to test other
applications with similar communication strategies.

In order to evaluate our improvements to SEA, we needed a more powerful
way to express anomalies.
To this end, we
implemented a new programming language, CSLang.  CSLang is a concise, but
% introduction of transducers is abrupt and out of place here
% need to introduce it earlier
expressive, language for describing both situations indicative of
simulation opportunities and the transformations required to make
said simulations a reality.
We used CSlang to describe a set of XXX anomalies that can occur in
applications that use JSONRPC or XMLRPC and employed them against
suitable
applications that ranked highly on Debian's popularity contest.  This
exercise identified AAA new bugs in the tested applications.
What's more, these descriptions can be reused on
similar applications to expose more
bugs without additional test writing effort.

\preston{Some stuff here might be... ambitious to prove}
Additionally, we used CSLang to re-implement the
anomalies previously described in \preston{however you cite the first paper
here}.
Our recreations were dramatically more concise (up to XXX\% in some cases)
than their counterparts.  Side-by-side comparison shows that our new
descriptions are easier to understand, more maintainable, and simpler to
update as needs change.  This lowers SEA's barrier to entry allowing the
technique to be more easily used by a wider audience.
%%% say that we chose JSON/XMLRPC in order to demonstrate that the
%%% technique works on complex, application layer protocols rather than
%%% just super low level stuff like system calls

% I don't like the performance/speed comparison here.  Focus on usability
%%% SOMEWHERE we need to talk about how old mutators were tightly coupled
%%% to system calls and now we can write a mutator and run on whatever
%%% we ingest

The main contributions in this work can be summarized as follows:

\begin{itemize}

\item{We present an expanded version of the Simulating Environmental
  Anomalies (SEA) technique that can expose bugs both between an
    application and its environment and between application components that
    communicate with one another}

\item{We construct a new language, {\em CSlang},
  which allows for concise, but powerful, descriptions of anomalies.}

\item{We show that anomalies described by CSlang can be used to find bugs
  in real-world applications.}

\item{We prove the usability of CSLang by illustrating the ease with which
  anomalies can be constructed.}

\end{itemize}

